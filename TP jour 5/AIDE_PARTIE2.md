# Aide - Partie 2 : Conteneurisation des microservices

## Indices pour l'impl√©mentation

### 1. Service de Model Serving (gRPC)

#### G√©n√©ration des fichiers proto

```bash
# Installer grpcio-tools
pip install grpcio-tools

# G√©n√©rer les fichiers Python depuis le .proto
python -m grpc_tools.protoc \
    -I. \
    --python_out=. \
    --grpc_python_out=. \
    model_serving.proto

# Cela g√©n√®re :
# - model_serving_pb2.py (messages)
# - model_serving_pb2_grpc.py (services)
```

#### Impl√©mentation du serveur gRPC

```python
import grpc
from concurrent import futures
import tensorflow as tf
import numpy as np
import base64
from PIL import Image
import io
import logging

import model_serving_pb2
import model_serving_pb2_grpc

logger = logging.getLogger(__name__)

class ModelServingService(model_serving_pb2_grpc.ModelServingServicer):
    
    def __init__(self, model_path):
        # Charger le mod√®le TensorFlow
        self.model = tf.keras.models.load_model(model_path)
        self.class_names = ['car', 'truck', 'bus']
        logger.info(f"‚úÖ Mod√®le charg√© depuis {model_path}")
    
    def Predict(self, request, context):
        try:
            import time
            start_time = time.time()
            
            # 1. D√©coder l'image depuis base64
            image_bytes = base64.b64decode(request.image_data)
            image = Image.open(io.BytesIO(image_bytes))
            
            # 2. Pr√©traiter l'image
            image = image.resize((224, 224))
            image_array = np.array(image) / 255.0
            image_array = np.expand_dims(image_array, axis=0)
            
            # 3. Faire la pr√©diction
            predictions = self.model.predict(image_array, verbose=0)
            
            # 4. Cr√©er les d√©tections
            detections = []
            for idx, confidence in enumerate(predictions[0]):
                if confidence > 0.1:  # Seuil de confiance
                    detection = model_serving_pb2.Detection(
                        class_name=self.class_names[idx],
                        confidence=float(confidence),
                        bbox=model_serving_pb2.BoundingBox(
                            x_min=0.0, y_min=0.0,
                            x_max=1.0, y_max=1.0
                        )
                    )
                    detections.append(detection)
            
            # 5. Calculer le temps d'inf√©rence
            inference_time = (time.time() - start_time) * 1000  # en ms
            
            # 6. Logger pour l'audit
            logger.info(f"üìä Pr√©diction - ID: {request.request_id}, "
                       f"User: {request.user_id}, "
                       f"Time: {inference_time:.2f}ms")
            
            # 7. Retourner la r√©ponse
            return model_serving_pb2.PredictionResponse(
                request_id=request.request_id,
                detections=detections,
                model_version="v1.0.0",
                inference_time_ms=inference_time
            )
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la pr√©diction: {str(e)}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return model_serving_pb2.PredictionResponse()
    
    def HealthCheck(self, request, context):
        return model_serving_pb2.HealthCheckResponse(status="healthy")

def serve():
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    model_serving_pb2_grpc.add_ModelServingServicer_to_server(
        ModelServingService(model_path="/app/models/car_detector_v1.h5"),
        server
    )
    server.add_insecure_port('[::]:50051')
    server.start()
    logger.info("üöÄ Serveur gRPC d√©marr√© sur le port 50051")
    server.wait_for_termination()
```

#### Client gRPC pour tester

```python
import grpc
import base64
import model_serving_pb2
import model_serving_pb2_grpc

def test_grpc_prediction(image_path):
    # Cr√©er un canal gRPC
    channel = grpc.insecure_channel('localhost:50051')
    stub = model_serving_pb2_grpc.ModelServingStub(channel)
    
    # Lire et encoder l'image
    with open(image_path, 'rb') as f:
        image_data = base64.b64encode(f.read()).decode('utf-8')
    
    # Cr√©er la requ√™te
    request = model_serving_pb2.PredictionRequest(
        request_id="test-001",
        image_data=image_data,
        user_id="user-123"
    )
    
    # Appeler le service
    response = stub.Predict(request)
    
    # Afficher les r√©sultats
    print(f"Request ID: {response.request_id}")
    print(f"Model Version: {response.model_version}")
    print(f"Inference Time: {response.inference_time_ms:.2f}ms")
    print("Detections:")
    for detection in response.detections:
        print(f"  - {detection.class_name}: {detection.confidence:.2%}")

if __name__ == "__main__":
    test_grpc_prediction("test_image.jpg")
```

### 2. Service de Features (REST)

#### Impl√©mentation compl√®te

```python
from flask import Flask, request, jsonify
import numpy as np
from PIL import Image
import io
import base64
import logging
from datetime import datetime

app = Flask(__name__)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def preprocess_image(image_bytes, target_size=(224, 224)):
    """Pr√©traite une image pour l'inf√©rence"""
    # D√©coder l'image
    image = Image.open(io.BytesIO(image_bytes))
    
    # Convertir en RGB si n√©cessaire
    if image.mode != 'RGB':
        image = image.convert('RGB')
    
    # Redimensionner
    image = image.resize(target_size)
    
    # Convertir en bytes
    buffer = io.BytesIO()
    image.save(buffer, format='JPEG')
    preprocessed_bytes = buffer.getvalue()
    
    # Encoder en base64
    encoded = base64.b64encode(preprocessed_bytes).decode('utf-8')
    
    return encoded, image

def extract_features(image):
    """Extrait des features basiques de l'image"""
    image_array = np.array(image)
    
    features = {
        'width': image.width,
        'height': image.height,
        'format': image.format or 'JPEG',
        'mode': image.mode,
        'mean_brightness': float(np.mean(image_array)),
        'std_brightness': float(np.std(image_array)),
        'min_pixel': int(np.min(image_array)),
        'max_pixel': int(np.max(image_array))
    }
    
    return features

@app.route('/health', methods=['GET'])
def health():
    return jsonify({
        "status": "healthy",
        "service": "feature-service",
        "timestamp": datetime.utcnow().isoformat()
    })

@app.route('/preprocess', methods=['POST'])
def preprocess():
    try:
        data = request.get_json()
        
        if not data or 'image' not in data:
            return jsonify({"error": "Image data required"}), 400
        
        request_id = data.get('request_id', 'unknown')
        
        # D√©coder l'image
        image_bytes = base64.b64decode(data['image'])
        
        # Pr√©traiter
        preprocessed_image, image = preprocess_image(image_bytes)
        
        # Extraire les features
        features = extract_features(image)
        
        # Logger
        logger.info(f"‚úÖ Image pr√©trait√©e - ID: {request_id}, "
                   f"Size: {features['width']}x{features['height']}")
        
        return jsonify({
            "preprocessed_image": preprocessed_image,
            "features": features,
            "request_id": request_id,
            "timestamp": datetime.utcnow().isoformat()
        })
        
    except Exception as e:
        logger.error(f"‚ùå Erreur de pr√©traitement: {str(e)}")
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5001, debug=False)
```

#### Test du Feature Service

```bash
# Test avec curl
curl -X POST http://localhost:5001/preprocess \
  -H "Content-Type: application/json" \
  -d '{
    "image": "'$(base64 -i test_image.jpg)'",
    "request_id": "test-001"
  }'
```

```python
# Test avec Python
import requests
import base64

def test_feature_service(image_path):
    with open(image_path, 'rb') as f:
        image_data = base64.b64encode(f.read()).decode('utf-8')
    
    response = requests.post(
        'http://localhost:5001/preprocess',
        json={
            'image': image_data,
            'request_id': 'test-001'
        }
    )
    
    print(response.json())

test_feature_service('test_image.jpg')
```

### 3. Service de Results (REST)

#### Impl√©mentation compl√®te avec SQLite

```python
from flask import Flask, request, jsonify
from datetime import datetime
import json
import sqlite3
import logging
import os

app = Flask(__name__)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

DB_PATH = os.getenv('DB_PATH', '/app/data/predictions.db')

def get_db_connection():
    """Cr√©e une connexion √† la base de donn√©es"""
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row  # Pour acc√©der aux colonnes par nom
    return conn

def init_db():
    """Initialise la base de donn√©es"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS predictions (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            request_id TEXT UNIQUE NOT NULL,
            user_id TEXT NOT NULL,
            predictions TEXT NOT NULL,
            model_version TEXT NOT NULL,
            inference_time_ms REAL NOT NULL,
            timestamp TEXT NOT NULL
        )
    ''')
    
    # Index pour les recherches par user_id
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_user_id ON predictions(user_id)
    ''')
    
    conn.commit()
    conn.close()
    logger.info("‚úÖ Base de donn√©es initialis√©e")

@app.route('/health', methods=['GET'])
def health():
    return jsonify({
        "status": "healthy",
        "service": "results-service",
        "timestamp": datetime.utcnow().isoformat()
    })

@app.route('/predictions', methods=['POST'])
def store_prediction():
    try:
        data = request.get_json()
        
        # Validation
        required_fields = ['request_id', 'user_id', 'predictions', 
                          'model_version', 'inference_time_ms']
        for field in required_fields:
            if field not in data:
                return jsonify({"error": f"Missing field: {field}"}), 400
        
        # Connexion √† la DB
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Insertion
        cursor.execute('''
            INSERT INTO predictions 
            (request_id, user_id, predictions, model_version, inference_time_ms, timestamp)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            data['request_id'],
            data['user_id'],
            json.dumps(data['predictions']),
            data['model_version'],
            data['inference_time_ms'],
            datetime.utcnow().isoformat()
        ))
        
        conn.commit()
        conn.close()
        
        logger.info(f"‚úÖ Pr√©diction stock√©e - ID: {data['request_id']}, "
                   f"User: {data['user_id']}")
        
        return jsonify({
            "status": "success",
            "request_id": data['request_id']
        }), 201
        
    except sqlite3.IntegrityError:
        return jsonify({"error": "Request ID already exists"}), 409
    except Exception as e:
        logger.error(f"‚ùå Erreur de stockage: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/predictions/<request_id>', methods=['GET'])
def get_prediction(request_id):
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT * FROM predictions WHERE request_id = ?
        ''', (request_id,))
        
        row = cursor.fetchone()
        conn.close()
        
        if row is None:
            return jsonify({"error": "Prediction not found"}), 404
        
        return jsonify({
            "request_id": row['request_id'],
            "user_id": row['user_id'],
            "predictions": json.loads(row['predictions']),
            "model_version": row['model_version'],
            "inference_time_ms": row['inference_time_ms'],
            "timestamp": row['timestamp']
        })
        
    except Exception as e:
        logger.error(f"‚ùå Erreur de r√©cup√©ration: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/predictions/user/<user_id>', methods=['GET'])
def get_user_predictions(user_id):
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT * FROM predictions WHERE user_id = ? ORDER BY timestamp DESC
        ''', (user_id,))
        
        rows = cursor.fetchall()
        conn.close()
        
        predictions = []
        for row in rows:
            predictions.append({
                "request_id": row['request_id'],
                "predictions": json.loads(row['predictions']),
                "model_version": row['model_version'],
                "inference_time_ms": row['inference_time_ms'],
                "timestamp": row['timestamp']
            })
        
        logger.info(f"üìä R√©cup√©ration de {len(predictions)} pr√©dictions "
                   f"pour l'utilisateur: {user_id}")
        
        return jsonify({
            "user_id": user_id,
            "count": len(predictions),
            "predictions": predictions
        })
        
    except Exception as e:
        logger.error(f"‚ùå Erreur de r√©cup√©ration: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/predictions/user/<user_id>', methods=['DELETE'])
def delete_user_predictions(user_id):
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Compter les enregistrements avant suppression
        cursor.execute('SELECT COUNT(*) FROM predictions WHERE user_id = ?', (user_id,))
        count = cursor.fetchone()[0]
        
        # Supprimer
        cursor.execute('DELETE FROM predictions WHERE user_id = ?', (user_id,))
        conn.commit()
        conn.close()
        
        logger.info(f"üóëÔ∏è {count} pr√©dictions supprim√©es pour l'utilisateur: {user_id} "
                   f"(Droit √† l'oubli RGPD)")
        
        return jsonify({
            "status": "success",
            "user_id": user_id,
            "deleted_count": count
        })
        
    except Exception as e:
        logger.error(f"‚ùå Erreur de suppression: {str(e)}")
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    init_db()
    app.run(host='0.0.0.0', port=5002, debug=False)
```

### 4. Docker Compose - Configuration compl√®te

```yaml
version: '3.8'

services:
  model-serving:
    build: ./services/model_serving
    container_name: model-serving
    ports:
      - "50051:50051"
    volumes:
      - ./models:/app/models:ro
    environment:
      - MODEL_PATH=/app/models/car_detector_v1.h5
      - LOG_LEVEL=INFO
    networks:
      - ia-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import grpc; print('OK')"]
      interval: 30s
      timeout: 10s
      retries: 3

  feature-service:
    build: ./services/feature_service
    container_name: feature-service
    ports:
      - "5001:5001"
    environment:
      - LOG_LEVEL=INFO
    networks:
      - ia-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  results-service:
    build: ./services/results_service
    container_name: results-service
    ports:
      - "5002:5002"
    volumes:
      - ./data/results:/app/data
    environment:
      - LOG_LEVEL=INFO
      - DB_PATH=/app/data/predictions.db
    networks:
      - ia-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5002/health"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  ia-network:
    driver: bridge
```

### 5. Commandes de test

```bash
# Construire et d√©marrer
docker-compose up --build -d

# V√©rifier les logs
docker-compose logs -f

# Tester les health checks
curl http://localhost:5001/health
curl http://localhost:5002/health

# Tester le feature service
curl -X POST http://localhost:5001/preprocess \
  -H "Content-Type: application/json" \
  -d '{"image": "base64_encoded_image", "request_id": "test-001"}'

# Tester le results service
curl -X POST http://localhost:5002/predictions \
  -H "Content-Type: application/json" \
  -d '{
    "request_id": "test-001",
    "user_id": "user-123",
    "predictions": [{"class": "car", "confidence": 0.95}],
    "model_version": "v1",
    "inference_time_ms": 45.2
  }'

# R√©cup√©rer une pr√©diction
curl http://localhost:5002/predictions/test-001

# Arr√™ter les services
docker-compose down
```

## Probl√®mes courants et solutions

### Probl√®me 1 : Erreur de build Docker

```bash
# V√©rifier les logs de build
docker-compose build --no-cache

# V√©rifier l'espace disque
docker system df

# Nettoyer les images inutilis√©es
docker system prune -a
```

### Probl√®me 2 : Service ne d√©marre pas

```bash
# V√©rifier les logs
docker-compose logs [service-name]

# V√©rifier les ports
netstat -an | grep LISTEN

# Red√©marrer un service sp√©cifique
docker-compose restart [service-name]
```

### Probl√®me 3 : Communication entre services

```bash
# Tester la connectivit√© r√©seau
docker-compose exec feature-service ping model-serving

# V√©rifier le r√©seau Docker
docker network ls
docker network inspect [network-name]
```

## Checklist Partie 2

- [ ] Service Model Serving impl√©ment√© et fonctionnel
- [ ] Service Feature Service impl√©ment√© et fonctionnel
- [ ] Service Results Service impl√©ment√© et fonctionnel
- [ ] Dockerfiles cr√©√©s pour chaque service
- [ ] docker-compose.yml configur√©
- [ ] Tous les services d√©marrent sans erreur
- [ ] Health checks fonctionnels
- [ ] Communication entre services test√©e
- [ ] Logs visibles et informatifs

## Temps estim√©

- Impl√©mentation Model Serving : 15 min
- Impl√©mentation Feature Service : 10 min
- Impl√©mentation Results Service : 15 min
- Configuration Docker : 5 min
